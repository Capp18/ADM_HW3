{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART1 (Fetching Urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "\n",
    "\"\"\"\n",
    "THIS SCRIPT IS FOR DOWNLOADING THE HTML FILES OF THE GIVEN DOCUMENTS\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(open(\"movies1.html\"), features=\"lxml\") # create a soup object\n",
    "url_list = []\n",
    "for url in soup.findAll('a', href=True): # go through all links in the html file\n",
    "    url_list.append(url['href']) # append urls ot a list\n",
    "a = 9430, 9671\n",
    "i = 0\n",
    "while i <= (len(url_list)): # loop through all list and download the links as html files\n",
    "    link = url_list[i]\n",
    "    try:\n",
    "        response = urllib.request.urlopen(link)\n",
    "        webContent = response.read()\n",
    "        with open(\"movies\\\\article_\"+str(i)+\".html\", \"wb\")as file:\n",
    "            file.write(webContent)\n",
    "        time.sleep(randint(1, 5))\n",
    "        print(\"in try \", i)\n",
    "        i += 1\n",
    "    except:\n",
    "        print(\"in except \", i)\n",
    "        time.sleep(120)\n",
    "\n",
    "# the files with number 9429 and 9670 can not be downloaded because of an unexpected error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART2 (Parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Onur\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:107: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "C:\\Users\\Onur\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:81: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d18350813a5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mpar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_element\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# clean it with clean text function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIntro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# Parse the Plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d18350813a5d>\u001b[0m in \u001b[0;36mclean_text\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# define a function to clean by using the nltk library\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\w+\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mporter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     23\u001b[0m         return [\n\u001b[0;32m     24\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         ]\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No such file or directory: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "\"\"\"\n",
    "This script is for parsing the html pages\n",
    "for each html page;\n",
    "    *The title\n",
    "    *Intro\n",
    "    *plot\n",
    "are fetched and cleaned by removing stopwords and punctuation\n",
    "\n",
    "After that, some relevant information are fetched from the Info box of each html file\n",
    "\n",
    "At the end, the extracted information is a stored as tsv files for each film \n",
    "\"\"\"\n",
    "\n",
    "# create a Data Frame to store the information for each film\n",
    "columns = [\"title\", \"intro\", \"plot\", \"film_name\", \"director\", \"producer\", \"writer\", \"starring\", \"music\", \"release_date\",\"running time\", \"country\", \"language\", \"budget\"]\n",
    "a = np.empty((10000, 14,))\n",
    "a[:] = np.nan\n",
    "df = pd.DataFrame(data=a, columns=columns)\n",
    "\n",
    "# define a function to clean by using the nltk library\n",
    "def clean_text(sentence):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = RegexpTokenizer(r\"\\w+\")\n",
    "    porter = PorterStemmer()\n",
    "    stem_words = list(map(porter.stem, tokens.tokenize(sentence)))\n",
    "    words = filter(lambda x: x not in string.punctuation, stem_words)\n",
    "    cleaned_text = filter(lambda x: x not in stop_words, words)\n",
    "    return cleaned_text\n",
    "\n",
    "# Loop through each film and record the relevant info to the Data Frame\n",
    "for film in range(10000): # this is for the first file of movies\n",
    "    if film in [9429, 9671]:\n",
    "        continue # the files with number 9429 and 9670 can not be downloaded because of an unexpected error because of that, they are excluded from the loop\n",
    "\n",
    "    soup = BeautifulSoup(open(\"movies\\\\article_\"+str(film)+\".html\", encoding=\"utf8\"), \"html.parser\")\n",
    "    #exctract the title\n",
    "    df.iloc[film, 0] = soup.find(\"h1\").text\n",
    "    df.iloc[film, 3] = soup.find(\"h1\").text\n",
    "    #extract the Intro\n",
    "    Intro = \"\"\n",
    "    par = soup.p # first paragraph of the html page\n",
    "    while par.next_element.name != \"h2\" and par.next_element.name != \"h3\": # until a heading, combine all paragraphs\n",
    "        if par.name == \"p\":\n",
    "            Intro += par.get_text()\n",
    "        par = par.next_element\n",
    "    # clean it with clean text function\n",
    "    df.iloc[film, 1] = \" \".join(list(clean_text(Intro)))\n",
    "\n",
    "    # Parse the Plot\n",
    "    for heading in soup.find_all([\"h2\", \"h3\"]):\n",
    "        try:\n",
    "            if (heading.contents[0].get(\"id\") == \"Plot\" or heading.contents[0].get(\"id\") == \"Plot_summary\" or\n",
    "                heading.contents[0].get(\"id\") == \"Plot_Summary\" or\n",
    "                    heading.contents[0].get(\"id\") == \"Premise\"): # first find the plot heading that can have different id's\n",
    "                break\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    Plot = \"\"\n",
    "    try:\n",
    "        while heading.next_element.name != \"h2\" and heading.next_element.name != \"h3\": # starting from plot heading, concatanete all paragraphs\n",
    "            if heading.name == \"p\":\n",
    "                Plot += heading.get_text()\n",
    "            heading = heading.next_element\n",
    "        # clean it with clean text function\n",
    "        df.iloc[film, 2] = \" \".join(list(clean_text(Plot)))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    # Info box Parsing\n",
    "\n",
    "    info_box = soup.find(\"table\", {\"class\": \"infobox vevent\"})\n",
    "    if info_box is None:\n",
    "        df.iloc[film, :].to_csv(\"parsed_clean\\\\\" + str(film) + \".tsv\", sep='\\t', encoding='utf-8')\n",
    "        continue\n",
    "    tags = info_box.contents[0].contents # find tags in infobox\n",
    "    for tr in tags:\n",
    "        if len(tr.contents) == 2:\n",
    "            flag = True\n",
    "            s = tr.contents[0].get_text() # get the headings in infobox\n",
    "            for i in range(len(columns)):\n",
    "                sub_Str = s[:4].lower()\n",
    "                target_str = columns[i]\n",
    "                if sub_Str in target_str: # find correct column to write the relevant information\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                continue\n",
    "            l = []\n",
    "            if len(tr.contents[1].contents) > 1:\n",
    "                for j in tr.contents[1].contents:\n",
    "                    if j.string is not None:\n",
    "                        l.append(j.string)\n",
    "                df.iloc[film, i] = \" \".join(l)\n",
    "            else:\n",
    "                df.iloc[film, i] = [tr.contents[1].get_text()]\n",
    "\n",
    "\n",
    "\n",
    "    df.iloc[film, :].to_csv(\"parsed_clean\\\\\"+str(film)+\".tsv\", sep='\\t', encoding='utf-8') # create a different folder called parsed and save the tsv files in it\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART3(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ephraim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>featur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kleiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>letterhead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spokeswoman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>barti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>octaroon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>eller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ehhh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>undisput</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mcclellan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>interlochen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>westmor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sempter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>beagl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fumig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>molokai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>qing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>surchart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>costain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>honeycutt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>jerald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>slower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>vorobyaninov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>passersbi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47181</th>\n",
       "      <td>goodal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47182</th>\n",
       "      <td>dachshund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47183</th>\n",
       "      <td>cartesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47184</th>\n",
       "      <td>imag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47185</th>\n",
       "      <td>baysvil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47186</th>\n",
       "      <td>freida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47187</th>\n",
       "      <td>21st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47188</th>\n",
       "      <td>nyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47189</th>\n",
       "      <td>couch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47190</th>\n",
       "      <td>jackpin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47191</th>\n",
       "      <td>cocoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47192</th>\n",
       "      <td>bandstand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47193</th>\n",
       "      <td>wayfar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47194</th>\n",
       "      <td>culley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47195</th>\n",
       "      <td>welder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47196</th>\n",
       "      <td>sovern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47197</th>\n",
       "      <td>merlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47198</th>\n",
       "      <td>pretti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47199</th>\n",
       "      <td>swindler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47200</th>\n",
       "      <td>extra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47201</th>\n",
       "      <td>bijou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47202</th>\n",
       "      <td>krofta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47203</th>\n",
       "      <td>casi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47204</th>\n",
       "      <td>mescalero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47205</th>\n",
       "      <td>patrimoni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47206</th>\n",
       "      <td>fuch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47207</th>\n",
       "      <td>cliveden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47208</th>\n",
       "      <td>beam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47209</th>\n",
       "      <td>moton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47210</th>\n",
       "      <td>brookhaven</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47211 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "0              huer\n",
       "1           ephraim\n",
       "2            featur\n",
       "3           kleiner\n",
       "4        letterhead\n",
       "5       spokeswoman\n",
       "6             barti\n",
       "7              chip\n",
       "8          octaroon\n",
       "9               tau\n",
       "10            eller\n",
       "11             ehhh\n",
       "12             nito\n",
       "13         undisput\n",
       "14        mcclellan\n",
       "15      interlochen\n",
       "16          westmor\n",
       "17          sempter\n",
       "18            beagl\n",
       "19            fumig\n",
       "20          molokai\n",
       "21             qing\n",
       "22         surchart\n",
       "23          costain\n",
       "24            stark\n",
       "25        honeycutt\n",
       "26           jerald\n",
       "27           slower\n",
       "28     vorobyaninov\n",
       "29        passersbi\n",
       "...             ...\n",
       "47181        goodal\n",
       "47182     dachshund\n",
       "47183     cartesian\n",
       "47184          imag\n",
       "47185       baysvil\n",
       "47186        freida\n",
       "47187          21st\n",
       "47188          nyle\n",
       "47189         couch\n",
       "47190       jackpin\n",
       "47191         cocoa\n",
       "47192     bandstand\n",
       "47193        wayfar\n",
       "47194        culley\n",
       "47195        welder\n",
       "47196        sovern\n",
       "47197        merlin\n",
       "47198        pretti\n",
       "47199      swindler\n",
       "47200         extra\n",
       "47201         bijou\n",
       "47202        krofta\n",
       "47203          casi\n",
       "47204     mescalero\n",
       "47205     patrimoni\n",
       "47206          fuch\n",
       "47207      cliveden\n",
       "47208          beam\n",
       "47209         moton\n",
       "47210    brookhaven\n",
       "\n",
       "[47211 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\"\"\"\n",
    "this script creates a vocabulary csv which contains the whole words contained\n",
    "in the html files\n",
    "\"\"\"\n",
    "text =\"\"\n",
    "for film in range(10000):\n",
    "    if film in [9429, 9671]: # the files with number 9429 and 9670 can not be downloaded because of an unexpected error because of that, they are excluded from the loop\n",
    "        continue\n",
    "    df = pd.read_csv(\"parsed_clean\\\\\"+str(film)+\".tsv\", sep='\\t', encoding='utf-8') #vocab read csv files\n",
    "    df = df.fillna(\"\")\n",
    "    # get the intro and plot and combine them.\n",
    "    intro = df.iloc[0, 1]\n",
    "    plot = df.iloc[1, 1]\n",
    "    text = text + \" \" + intro + \" \" + plot #concatanete all intros and plots for all films\n",
    "\n",
    "arr = set(text.split()) # use set to eliminate repeating words\n",
    "vocab = pd.DataFrame(arr)\n",
    "vocab.to_csv(\"vocab.csv\") #save vocabulary as csv with indices for each\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART4(inverted indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\"\"\"\n",
    "This script creates inverted indices file for each word in vocabulary.csv\n",
    "\n",
    "\"\"\"\n",
    "vocab = pd.read_csv(\"vocab.csv\")\n",
    "inverted_indices = {}\n",
    "vocab = vocab.set_index(\"0\").to_dict()[\"Unnamed: 0\"] # set the keys of the inverted indices dictionary as the id's in vocabulary.csv\n",
    "for index in range(len(vocab.values())):\n",
    "    inverted_indices[index] = [] # set the values of inverted indices dictionary empty lists, later the documents containig relevant word\n",
    "                                # would be appended to this list\n",
    "for film in range(10000):\n",
    "    text=\"\"\n",
    "    if film in [9429, 9671]:\n",
    "        continue\n",
    "    # get the intro + plot of each film as text\n",
    "    df = pd.read_csv(\"parsed_clean\\\\\"+str(film)+\".tsv\", sep='\\t', encoding='utf-8')\n",
    "    df = df.fillna(\"\")\n",
    "    intro = df.iloc[0, 1]\n",
    "    plot = df.iloc[1, 1]\n",
    "    text = intro + \" \" + plot\n",
    "    # go through each word in the text and ad the document to the value in the inverted indicies dictionary\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            term_id =int(vocab[word.lower()])\n",
    "            inverted_indices[term_id].append(\"doc_\"+str(film))\n",
    "        except KeyError:\n",
    "            continue\n",
    "# save the dictionary as csv\n",
    "import csv\n",
    "with open('inverted_indices.csv', \"w\") as f:  # Just use 'w' mode in 3.x\n",
    "    w = csv.DictWriter(f, inverted_indices.keys())\n",
    "    w.writeheader()\n",
    "    w.writerow(inverted_indices)\n",
    "\n",
    "op=pd.read_csv(\"inverted_indices.csv\").transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inverted_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART5(first search engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Title  \\\n",
      "0                      The Creeping Terror   \n",
      "1                      The Egyptian (film)   \n",
      "2         His Majesty, the Scarecrow of Oz   \n",
      "3                           The Wiz (film)   \n",
      "4   The Wonderful Wizard of Oz (1910 film)   \n",
      "5                         The 300 Spartans   \n",
      "6                            Paper Bullets   \n",
      "7           You Never Can Tell (1951 film)   \n",
      "8             The Adventures of Marco Polo   \n",
      "9          Nothing but Trouble (1944 film)   \n",
      "10                     Merry Andrew (film)   \n",
      "11                      White Witch Doctor   \n",
      "12         Douglas Fairbanks in Robin Hood   \n",
      "13                         One More Spring   \n",
      "14                         The Lion (film)   \n",
      "15                A Lion Is in the Streets   \n",
      "16               Tarzan and the Slave Girl   \n",
      "17   A Midsummer Night's Dream (1935 film)   \n",
      "18                 Tarzan and the Huntress   \n",
      "19            Mighty Joe Young (1949 film)   \n",
      "\n",
      "                                                Intro  \\\n",
      "0   \\nThe Creeping Terror (a.k.a. The Crawling Mon...   \n",
      "1   The Egyptian is a 1954 American epic drama fil...   \n",
      "2   \\nHis Majesty, the Scarecrow of Oz is a 1914 A...   \n",
      "3   \\nThe Wiz is a 1978 American musical adventure...   \n",
      "4   The Wonderful Wizard of Oz is a 1910 American ...   \n",
      "5   The 300 Spartans is a 1962 CinemaScope epic fi...   \n",
      "6   Paper Bullets is a 1941 American film directed...   \n",
      "7   You Never Can Tell is 1951 American black and ...   \n",
      "8   The Adventures of Marco Polo is a 1938 adventu...   \n",
      "9   Nothing But Trouble is a 1944 Laurel and Hardy...   \n",
      "10  Merry Andrew is a 1958 American musical film d...   \n",
      "11  White Witch Doctor is a 1953 Technicolor adven...   \n",
      "12  Robin Hood is a 1922 adventure film starring D...   \n",
      "13  One More Spring is a 1935 American comedy dram...   \n",
      "14  The Lion is a DeLuxe Color 1962 drama film in ...   \n",
      "15  A Lion Is in the Streets is a 1953 drama film ...   \n",
      "16  Tarzan and the Slave Girl is a 1950 film direc...   \n",
      "17  A Midsummer Night's Dream is a 1935 American r...   \n",
      "18  Tarzan and the Huntress is a 1947 adventure fi...   \n",
      "19  Mighty Joe Young (also known as Mr. Joseph You...   \n",
      "\n",
      "                                                  url  \n",
      "0   https://en.wikipedia.org/wiki/The_Creeping_Terror  \n",
      "1   https://en.wikipedia.org/wiki/The_Egyptian_(film)  \n",
      "2   https://en.wikipedia.org/wiki/His_Majesty,_the...  \n",
      "3        https://en.wikipedia.org/wiki/The_Wiz_(film)  \n",
      "4   https://en.wikipedia.org/wiki/The_Wonderful_Wi...  \n",
      "5      https://en.wikipedia.org/wiki/The_300_Spartans  \n",
      "6         https://en.wikipedia.org/wiki/Paper_Bullets  \n",
      "7   https://en.wikipedia.org/wiki/You_Never_Can_Te...  \n",
      "8   https://en.wikipedia.org/wiki/The_Adventures_o...  \n",
      "9   https://en.wikipedia.org/wiki/Nothing_But_Trou...  \n",
      "10  https://en.wikipedia.org/wiki/Merry_Andrew_(film)  \n",
      "11  https://en.wikipedia.org/wiki/White_Witch_Doct...  \n",
      "12  https://en.wikipedia.org/wiki/Robin_Hood_(1922...  \n",
      "13      https://en.wikipedia.org/wiki/One_More_Spring  \n",
      "14      https://en.wikipedia.org/wiki/The_Lion_(film)  \n",
      "15  https://en.wikipedia.org/wiki/A_Lion_Is_in_the...  \n",
      "16  https://en.wikipedia.org/wiki/Tarzan_and_the_S...  \n",
      "17  https://en.wikipedia.org/wiki/A_Midsummer_Nigh...  \n",
      "18  https://en.wikipedia.org/wiki/Tarzan_and_the_H...  \n",
      "19  https://en.wikipedia.org/wiki/Mighty_Joe_Young...  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script is the first search engine without a score\n",
    "returns title, intro and url for relevant films\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "# get the urls of each movie\n",
    "soup = BeautifulSoup(open(\"movies1.html\"), features=\"lxml\")\n",
    "url_list = []\n",
    "for url in soup.findAll('a', href=True):\n",
    "    url_list.append(url['href'])\n",
    "\n",
    "# import the vocab csv\n",
    "vocab = pd.read_csv(\"vocab.csv\")\n",
    "vocab = vocab.set_index(\"0\").to_dict()[\"Unnamed: 0\"]\n",
    "op = pd.read_csv(\"inverted_indices.csv\").transpose()\n",
    "\n",
    "sentence = \"lion king\"\n",
    "\n",
    "htmls = []\n",
    "for word in sentence.split(): # go trough each word in sentence and match the word with id's in vocab csv\n",
    "    try:\n",
    "        id = vocab[word]\n",
    "    except KeyError:\n",
    "        word = word[:-1] # sometimes the last letter of a word is droped when nltk library used because of an un known reason\n",
    "        try:            # to overcome this, the matching is done with eliminating the last letter\n",
    "            id = vocab[word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    doc = op.iloc[id].values[0].split()\n",
    "    doc[0] = doc[0][1:]\n",
    "    doc[-1] = doc[-1][:-1]\n",
    "    htmls.append(set(doc))\n",
    "\n",
    "# use set intersection to get the documents containing all words\n",
    "intersection = set.intersection(*htmls)\n",
    "# create a results dataframe to store the info\n",
    "results = pd.DataFrame({'Title': [], 'Intro': [], 'url': []})\n",
    "counter = 0\n",
    "for document in intersection:\n",
    "    document = re.findall(r'\\d+', document)[0]\n",
    "\n",
    "    soup = BeautifulSoup(open(\"movies\\\\article_\" + document + \".html\", encoding=\"utf8\"), \"html.parser\")\n",
    "    # get title ( same in parse)\n",
    "    title = soup.find(\"h1\").text\n",
    "    # get intro (same in parse)\n",
    "    Intro = \"\"\n",
    "    par = soup.p # first paragraph of the html page\n",
    "    while par.next_element.name != \"h2\" and par.next_element.name != \"h3\": # until a heading, combine all paragraphs\n",
    "        if par.name == \"p\":\n",
    "            Intro += par.get_text()\n",
    "        par = par.next_element\n",
    "    # get url ( same in fetch_urls)\n",
    "    url = url_list[int(document)]\n",
    "    results.loc[counter] = [title, Intro, url]\n",
    "    counter += 1\n",
    "\n",
    "print(results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
